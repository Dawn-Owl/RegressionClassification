import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import GroupKFold, StratifiedKFold, GridSearchCV, RepeatedStratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score
from scipy.stats import ttest_rel
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

%matplotlib inline

# Load data
telco = pd.read_csv('/content/sample_data/telco_customer.csv')

# 데이터 전처리 과정
telco = telco.drop('customerID', axis=1)
binary_columns = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']
telco[binary_columns] = telco[binary_columns].apply(lambda x: x.map({'Yes': 1, 'No': 0}))
multi_class_columns = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 
                       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 
                       'Contract', 'PaymentMethod']
telco = pd.get_dummies(telco, columns=multi_class_columns, drop_first=True)
telco['TotalCharges'] = pd.to_numeric(telco['TotalCharges'], errors='coerce')
telco['TotalCharges'] = telco['TotalCharges'].fillna(telco['TotalCharges'].mean())
telco['gender'] = telco['gender'].map({'Male': 1, 'Female': 0})
telco = telco.astype({col: 'int' for col in telco.select_dtypes('bool').columns})

# tenure가 0인 데이터를 제외
telco = telco[telco["tenure"] > 0]

# tenure와 관련된 특성 추가
telco["tenure_cat"] = pd.cut(telco["tenure"], bins=[0, 24, 48, 60, float('inf')], labels=[1, 2, 3, 4])
telco['tenure_log'] = np.log1p(telco['tenure'])
telco['tenure_MonthlyCharges_interaction'] = telco['tenure'] * telco['MonthlyCharges']

# 데이터프레임 인덱스 재설정
telco.reset_index(drop=True, inplace=True)

telco.info()

# GroupKFold로 훈련 세트와 테스트 세트 나누기
group_kfold = GroupKFold(n_splits=4)
for train_index, test_index in group_kfold.split(telco, groups=telco["tenure_cat"]):
    X_train, X_test = telco.loc[train_index], telco.loc[test_index]
    break

for set_ in (X_train, X_test):
    set_.drop("tenure_cat", axis=1, inplace=True)

# 특성과 타겟 변수 분리
X = X_train.drop(["Churn"], axis=1)
y = X_train["Churn"]

# SMOTE 설정
smote = SMOTE(sampling_strategy=0.5, random_state=42)  # Churn을 전체의 50% 비율로 샘플링

# 하이퍼파라미터 그리드 설정
param_grids = {
    'Logistic Regression': {'classifier__C': [0.01, 0.1, 1, 10]},
    'K-Neighbors': {'classifier__n_neighbors': [3, 5, 7]},
    'SVM': {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']},
    'Random Forest': {'classifier__n_estimators': [50, 100], 'classifier__max_depth': [10, 20], 'classifier__min_samples_leaf': [2, 4]},
    'Gradient Boosting': {'classifier__n_estimators': [50, 100], 'classifier__learning_rate': [0.01, 0.1]},
    'XGBoost': {'classifier__n_estimators': [50, 100], 'classifier__learning_rate': [0.01, 0.1]},
    'LightGBM': {'classifier__n_estimators': [50, 100], 'classifier__learning_rate': [0.01, 0.1]}
}

# 모델 정의 및 최적화
models = {
    'Logistic Regression': ImbPipeline([
        ('smote', smote),
        ('scaler', StandardScaler()), 
        ('classifier', LogisticRegression(class_weight='balanced', random_state=42))
    ]),
    'K-Neighbors': ImbPipeline([
        ('smote', smote),
        ('scaler', StandardScaler()), 
        ('classifier', KNeighborsClassifier())
    ]),
    'SVM': ImbPipeline([
        ('smote', smote),
        ('scaler', StandardScaler()), 
        ('classifier', SVC(class_weight='balanced', probability=True, random_state=42))
    ]),
    'Random Forest': ImbPipeline([
        ('smote', smote),
        ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))
    ]),
    'Gradient Boosting': ImbPipeline([
        ('smote', smote),
        ('classifier', GradientBoostingClassifier(random_state=42))
    ]),
    'XGBoost': ImbPipeline([
        ('smote', smote),
        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
    ]),
    'LightGBM': ImbPipeline([
        ('smote', smote),
        ('classifier', LGBMClassifier(random_state=42))
    ])
}

# 최적화 모델 저장
optimized_models = {}
for name, model in models.items():
    if name in param_grids:
        grid_search = GridSearchCV(model, param_grid=param_grids[name], scoring='f1', cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42), n_jobs=-1)
        grid_search.fit(X, y)
        optimized_models[name] = grid_search.best_estimator_
        print(f"{name} - Best Params: {grid_search.best_params_}")
    else:
        model.fit(X, y)
        optimized_models[name] = model

# Voting과 Stacking 모델 추가 (최적화된 모델 사용)
voting_soft = VotingClassifier(
    estimators=[(name, optimized_models[name]) for name in optimized_models.keys()],
    voting='soft'
)

stacking_clf = StackingClassifier(
    estimators=[(name, optimized_models[name]) for name in optimized_models.keys()],
    final_estimator=LogisticRegression()  # 메타 모델로 로지스틱 회귀 사용
)

optimized_models['Voting Ensemble (Soft)'] = voting_soft
optimized_models['Stacking Ensemble'] = stacking_clf

# 특성 중요도 시각화 (예시: Random Forest)
print("\nFeature Importance (Random Forest):")
rf_model = optimized_models['Random Forest']
rf_model.fit(X, y)
importances = rf_model.named_steps['classifier'].feature_importances_
feature_names = X.columns
feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)

# 상위 10개 특성 시각화
plt.figure(figsize=(10, 6))
feature_importances[:10].plot(kind='barh')
plt.title('Top 10 Feature Importances (Random Forest)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# K-fold 교차 검증 설정
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 모델별 성능 평가 (AUC 및 f1-score 기준 5-fold 교차 검증)
model_auc_scores = {name: [] for name in optimized_models.keys()}
model_f1_scores = {name: [] for name in optimized_models.keys()}
for name, model in optimized_models.items():
    print(f"\nEvaluating {name} with 5-fold cross-validation on AUC and F1-score:\n")
    auc_scores = []
    f1_scores = []
    
    for fold, (train_index, test_index) in enumerate(kf.split(X, y)):
        X_train_fold, X_valid_fold = X.iloc[train_index], X.iloc[test_index]
        y_train_fold, y_valid_fold = y.iloc[train_index], y.iloc[test_index]
        
        model.fit(X_train_fold, y_train_fold)
        y_pred = model.predict(X_valid_fold)
        y_proba = model.predict_proba(X_valid_fold)[:, 1] if hasattr(model, "predict_proba") else None
        
        # f1-score 계산
        f1 = f1_score(y_valid_fold, y_pred)
        f1_scores.append(f1)
        
        # AUC 계산
        if y_proba is not None:
            auc = roc_auc_score(y_valid_fold, y_proba)
            auc_scores.append(auc)
    
    model_auc_scores[name].extend(auc_scores)
    model_f1_scores[name].extend(f1_scores)

# 모델 간 F1-score 비교 (통계적 테스트)
base_model_name_f1, base_model_score_f1 = max(
    [(name, np.mean(scores)) for name, scores in model_f1_scores.items()],
    key=lambda x: x[1]
)
print(f"\nBest model based on F1-score: {base_model_name_f1} with F1 = {base_model_score_f1:.2f}")

for name, scores in model_f1_scores.items():
    if name != base_model_name_f1:
        stat, p_value = ttest_rel(model_f1_scores[base_model_name_f1], scores)
        print(f"\nF1-score Comparison with {name}: t-statistic = {stat:.2f}, p-value = {p_value:.4f}")
        print(f"{name} is {'significantly different' if p_value < 0.05 else 'not significantly different'} from {base_model_name_f1}")

# 모델 간 AUC 비교 (통계적 테스트)
base_model_name_auc, base_model_score_auc = max(
    [(name, np.mean(scores)) for name, scores in model_auc_scores.items()],
    key=lambda x: x[1]
)
print(f"\nBest model based on AUC: {base_model_name_auc} with AUC = {base_model_score_auc:.2f}")

for name, scores in model_auc_scores.items():
    if name != base_model_name_auc:
        stat, p_value = ttest_rel(model_auc_scores[base_model_name_auc], scores)
        print(f"\nAUC Comparison with {name}: t-statistic = {stat:.2f}, p-value = {p_value:.4f}")
        print(f"{name} is {'significantly different' if p_value < 0.05 else 'not significantly different'} from {base_model_name_auc}")

# 교차 검증 AUC 및 F1 점수 시각화 (Boxplot)
plt.figure(figsize=(28, 6))
plt.subplot(1, 2, 1)
auc_scores_df = pd.DataFrame(model_auc_scores)
sns.boxplot(data=auc_scores_df)
plt.title('Cross-Validation AUC Score Distribution by Model')
plt.xlabel('Model')
plt.ylabel('AUC Score')

plt.subplot(1, 2, 2)
f1_scores_df = pd.DataFrame(model_f1_scores)
sns.boxplot(data=f1_scores_df)
plt.title('Cross-Validation F1 Score Distribution by Model')
plt.xlabel('Model')
plt.ylabel('F1 Score')

plt.tight_layout()
plt.show()

#이제 테스트 부분
# 최종 성능 평가: 20% 테스트 세트로 성능 확인
X_test_final = X_test.drop("Churn", axis=1)  # 특성 데이터 (Churn 제외)
y_test_final = X_test["Churn"]               # 타겟 데이터 (Churn만 포함)

# 최종 테스트 세트 성능 기록
final_test_scores = {}
final_test_f1_scores = {}

# 최적화된 모델을 테스트 세트에서 평가
for name, model in optimized_models.items():
    print(f"\nEvaluating {name} on the test set:\n")
    y_pred_test = model.predict(X_test_final)
    y_proba_test = model.predict_proba(X_test_final)[:, 1] if hasattr(model, "predict_proba") else None
    
    # 최종 테스트 성능 측정
    print(classification_report(y_test_final, y_pred_test, target_names=["No Churn", "Churn"], zero_division=0))
    print(f"Confusion Matrix for {name}:")
    print(confusion_matrix(y_test_final, y_pred_test))
    
    # ROC AUC 계산 및 저장
    if y_proba_test is not None:
        auc_score = roc_auc_score(y_test_final, y_proba_test)
        final_test_scores[name] = auc_score
        print(f"{name} AUC on test set: {auc_score:.2f}")
        
        # ROC Curve 시각화
        fpr, tpr, _ = roc_curve(y_test_final, y_proba_test)
        plt.plot(fpr, tpr, label=f"{name} (AUC = {auc_score:.2f})")
    else:
        print(f"{name} does not support probability prediction (AUC and ROC skipped)")
    
    # f1-score 계산 및 저장
    f1 = f1_score(y_test_final, y_pred_test)
    final_test_f1_scores[name] = f1
    print(f"{name} F1 Score on test set: {f1:.2f}")

# ROC Curve 전체 시각화 설정
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve on Test Set')
plt.legend()
plt.show()

# 최종 테스트 AUC 및 F1 성능 출력
print("\nFinal AUC and F1 scores on the test set for all models:")
for name in final_test_scores.keys():
    print(f"{name}: AUC = {final_test_scores[name]:.2f}, F1 = {final_test_f1_scores[name]:.2f}")
